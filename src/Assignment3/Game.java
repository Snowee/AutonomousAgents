package Assignment3;
import java.awt.Point;
import java.io.FileNotFoundException;
import java.io.PrintWriter;
import java.io.UnsupportedEncodingException;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Random;
import java.util.Set;

public class Game {
	// Class variables for game
	private Predator pred;
	private Prey prey;
	private boolean endState;
	// Class variables for non reduced algorithms
	public List<Point> allPredPos;
	// Class variables for reduced algorithms
	public Map<Point, Map<Point, Double>> Qvalues;
	public Map<Point, Map<Point, Double>> Nvalues;
	public Map<Point, Map<Point, Double>> Dvalues;
	public Map<Point, Point> detPolicy;
	public Map<Point, String> bestPolicy;
	private Point[] statesArray;
	private List<Double> rewardList = new ArrayList<Double>();
	private Map<List<Point>, Boolean> appearanceList = 
			new HashMap<List<Point>, Boolean>();
	private Map<Point, Point> appearanceListS = 
			new HashMap<Point, Point>();
	Map<Point, Integer> stateCounts;
	
	// Constructor for a game object
	public Game( Predator pred, Prey prey ) {
		this.pred = pred;
		this.prey = prey;
		endState = false;
		// Initialize all data structures to be used
		Qvalues = new HashMap<Point, Map<Point, Double>>();
		Nvalues = new HashMap<Point, Map<Point, Double>>();
		Dvalues = new HashMap<Point, Map<Point, Double>>();
		detPolicy = new HashMap<Point, Point>();
		allPredPos = new ArrayList<Point>();
		bestPolicy = new HashMap<Point, String>();
		stateCounts = new HashMap<Point, Integer>();
	}
	
	// Function to run a single game, each turn starting by a move of the prey
	// then a move of the predator and then a check to see if the predator
	// caught the prey. 
	// Input: boolean randomPolicy - true for selecting random moves
	//								 false for good policy moves
	// 		  boolean reduction - true to use the reduced state space
	//							  false to use full state space
	public int start( boolean randomPolicy) {
		int counter = 0;
		printGameState( pred.pos, prey.pos );
		while( endState == false ) {			
			// Prey move
			String predNear = prey.checkPred( pred );
			String preyMove = prey.getMove( predNear );
			Point moveCoordsPrey = move( preyMove );
			prey.newPos( moveCoordsPrey );
			// Uncomment below for print statement to see movement of prey
			//printGameState(pred.pos, prey.pos);
			
			// Pred move
			String predMove;
			if( randomPolicy ) {
				// random move
				predMove = pred.getMove();
			}
			else {
				Point state = new Point();
				state.x = prey.pos.x - pred.pos.x;
				state.y = prey.pos.y - pred.pos.y;
				state = pred.checkDirections( state );
				predMove = bestPolicy.get( state );		
				}
			
			Point moveCoordsPred = move( predMove );
			pred.newPos( moveCoordsPred );
			// nr of steps
			counter += 1;
			
			//Uncomment below for print statement to see movement of predator
			//printGameState( pred.pos, prey.pos );
			
			//check if end of game
			checkStatus(pred, prey);
		}
		System.out.printf( "Game ended in %d steps\n", counter );
		return counter;
	}
	
	// Function to test the performance of the deterministic policy 
	// generated by off policy Monte Carlo
	public int start1( boolean randomPolicy) {
		Random rand = new Random();
		double eps = 0.4;
		int counter = 0;
		
		// predator and prey start at random positions
		// but not same position
		Prey prey = new Prey(true);
		Predator pred = new Predator(true);
		while(pred.pos.equals(prey.pos)){
			prey = new Prey(true);
			pred = new Predator(true);
		}
		
		while( endState == false ) {	
			// if game has gone on for 10000 moves, stop
			if(counter >= 10000){
				break;
			}
			// Prey move
			String predNear = prey.checkPred( pred );
			String preyMove = prey.getMove( predNear );
			Point moveCoordsPrey = move( preyMove );
			prey.newPos( moveCoordsPrey );

			// Pred move
			String predMove;
			Point moveCoordsPred = new Point();
			if( randomPolicy ) {
				// random move
				predMove = pred.getMove();
			}
			else {
				Point state = new Point();
				state.x = prey.pos.x - pred.pos.x;
				state.y = prey.pos.y - pred.pos.y;
				state = pred.checkDirections( state );
				// With eps (0.4) probability, pick one of the actions with
				// maximum Q value. Used to break through possible loop patterns.
				if(rand.nextDouble() < eps){
					List<Point> maxAct = new ArrayList<Point>();
					double maxVal = 0;
					for(Entry<Point, Double> actval : Qvalues.get(state).entrySet()){
						if(actval.getValue() == maxVal ){
							maxAct.add(actval.getKey());
						}
						else if(actval.getValue() > maxVal){
							maxAct.clear();
							maxAct.add(actval.getKey());
							maxVal = actval.getValue();
						}
					}
					
					moveCoordsPred = maxAct.get(rand.nextInt(maxAct.size()));
				}
				// With remaining 0.6 probability pick the action from
				// the deterministic policy
				else{
				moveCoordsPred = (Point) detPolicy.get(state).clone(); 
				}
			}
			
			pred.newPos( moveCoordsPred );
			// nr of steps
			counter += 1;

			//check if end of game
			checkStatus(pred, prey);
		}
		System.out.printf( "Game ended in %d steps\n", counter );
		endState = false;
		return counter;
	}
	
	// Function to print the Point pred and Point prey locations in a board
	public void printGameState( Point pred, Point prey ) {
		String[][] board = new String[11][11];
		board[prey.y][prey.x] = "q";
		board[pred.y][pred.x] = "P";
		for( String[] row : board ) {
	        for( String r : row ) {
	        	if( r == null )
	        		System.out.printf("-\t");
	        	else System.out.printf("%s\t", r);
	        }
	        System.out.println();
	    }
		System.out.println();
	}
	
	// Function to obtain a Point representation of a String move
	public Point move( String move ) {
		Point moveCoords = new Point();
		switch( move ) {
		case "NORTH":
			moveCoords.x = 0;
			moveCoords.y = -1;
			break;
		case "EAST":
			moveCoords.x = 1;
			moveCoords.y = 0;
			break;
		case "SOUTH":
			moveCoords.x = 0;
			moveCoords.y = 1;
			break;
		case "WEST":
			moveCoords.x = -1;
			moveCoords.y = 0;
			break;
		case "WAIT":
			moveCoords.x = 0;
			moveCoords.y = 0;
			break;
		default:
			break;
		}
		return moveCoords;
	}
	
	// Check if game should end
	public void checkStatus(Predator pred, Prey prey) {
		if( pred.pos.equals( prey.pos ) ) {
			endState = true;
		}
	}
	
	// Calculate the manhattan distance given the reduced state space
	public int calcDistance( Point state ) {
		return Math.abs( state.x ) + Math.abs( state.y );
	}
	
	// Check if a point is inside the board, if not alter the coordinates to
	// match the toroidal environment
	public Point checkLoc( Point loc ) {
		if( !( loc.x >= 0 ) || !( loc.x < 11 ) ) {
			loc.x = ( loc.x + 11 ) % 11;
		}
		if( !( loc.y >= 0 ) || !( loc.y < 11 ) ) {
				loc.y = ( loc.y + 11 ) % 11;
		}
		return loc;
	}
	
	/*
	// Function for reduced initialization of arbitrary function
	// ( all states, "west" )
	public Map<Point, String> reductionInitPolicy() {
		Map<Point, String> initialPolicy = new HashMap<Point, String>();
		String move = "WEST";
		for( int i = -5; i < 6; i++ ) {
			for( int j = -5; j < 6; j++ ) {
				Point state = new Point( i, j );
				initialPolicy.put( state, move );
			}
		}
		return initialPolicy;
	}*/
	
	// Function to print a board with actions on its coordinates
	public void printBoardActions( Map<Point, String> map, 
			Point preyLoc ) {
		String[][] board = new String[11][11];
		for( Map.Entry<Point, String> entry : map.entrySet() ) {
			Point state = entry.getKey();
			String action = map.get( state );
			Point newState = (Point) preyLoc.clone();
			newState.x -= state.x;
			newState.y -= state.y;
			newState = checkLoc( newState );
			board[newState.y][newState.x] = action;
		}
		for( String[] row : board ) {
	        for( String r : row ) {
	        	System.out.printf( "%s\t", r );
	        }
	        System.out.println();
	    }
		System.out.println();
	}
	
	// Function to print the values for all states, based on reduced state space
	public void printBoard( Map<Point, Double> map, Point preyLoc ) {
		double[][] board = new double[11][11];
		for( Map.Entry<Point, Double> entry : map.entrySet() ) {
			Point state = entry.getKey();
			double value = entry.getValue();
			Point nextLoc = (Point) preyLoc.clone();
			nextLoc.x -= state.x;
			nextLoc.y -= state.y;
			nextLoc = checkLoc( nextLoc );
			board[nextLoc.y][nextLoc.x] = value;
		}
		for( double[] row : board ) {
	        for( double r : row ) {
	        	if( r == 0 )
	        		System.out.print( "0         " );
	        	else
	        		System.out.printf( "%.9f", r );
	            System.out.print( "\t" );
	        }
	        System.out.println();
	    }
		System.out.println();
	}	
	
	// Function to print the maximum Qvalue for each state
	public void printboardQmax(Map<Point, Map<Point, Double>> map, Point preyLoc){
		double[][] board = new double[11][11];
		for( Map.Entry<Point,Map<Point, Double>> entry : map.entrySet() ) {
			Point state = entry.getKey();
			Map<Point, Double> value = entry.getValue();
			Point nextLoc = (Point) preyLoc.clone();
			nextLoc.x -= state.x;
			nextLoc.y -= state.y;
			nextLoc = checkLoc( nextLoc );
			double max = 0;
			// loop through the Q values per state to find the max
			for(int i = 0; i < pred.actions.length; i++){
				if(value.get(move(pred.actions[i])) > max ){
					max = value.get(move(pred.actions[i]));
				}
			}
			board[nextLoc.y][nextLoc.x] = max;
		}
		// print found values
		for( double[] row : board ) {
	        for( double r : row ) {
	        	System.out.printf( "%f\t", r );
	        }
	        System.out.println();
	    }
		System.out.println();
	}
	
	// Function to print the action that corresponds to the maximum
	// Q value in each state
	public void printboardQActions(Map<Point, Map<Point, Double>> map, Point preyLoc){
		String[][] board = new String[11][11];
		for( Map.Entry<Point,Map<Point, Double>> entry : map.entrySet() ) {
			Point state = entry.getKey();
			Map<Point, Double> value = entry.getValue();
			Point nextLoc = (Point) preyLoc.clone();
			nextLoc.x -= state.x;
			nextLoc.y -= state.y;
			nextLoc = checkLoc( nextLoc );
			String bestAction = "";
			if(!nextLoc.equals(preyLoc)){
				double max = 0;
				// get the max Q value and use that index to 
				// find the corresponding move
				for(int i = 0; i < pred.actions.length; i++){
					if(value.get(move(pred.actions[i])) > max ){
						bestAction = pred.actions[i];
						max = value.get(move(pred.actions[i]));
					}
				}
			}
			board[nextLoc.y][nextLoc.x] = bestAction;
		}
		// print all found moves
		for( String[] row : board ) {
	        for( String r : row ) {
	        	System.out.printf( "%s\t", r );
	        }
	        System.out.println();
	    }
		System.out.println();
	}

	// Print all five Qvalues per state, corresponding to the possible
	// actions of the predator in the order: WAIT, NORTH, EAST, SOUTH, WEST
	public void printBoardQ( Map<Point, Map<Point, Double>> map, Point preyLoc ) {
		double[][][] board = new double[11][11][5];
		for( Map.Entry<Point, Map<Point, Double>> entry : map.entrySet() ) {
			Point state = entry.getKey();
			Map<Point, Double> value = entry.getValue();
			Point nextLoc = (Point) preyLoc.clone();
			nextLoc.x -= state.x;
			nextLoc.y -= state.y;
			nextLoc = checkLoc( nextLoc );
			int counter = 0;
			for(int i = 0; i < pred.actions.length; i++){
				double value1 = value.get(move(pred.actions[i]));
				board[nextLoc.y][nextLoc.x][counter] = value1;
				counter++;
			}
		}
		// print all values
		for(int r = 0; r < 11; r++){
			for(int i = 0; i < 5; i++){
				for( int c = 0; c < 11; c++){
					if( board[r][c][i] == 0 )
		        		System.out.print( "0         \t" );
		        	else{
		        		System.out.printf( "%.9f", board[r][c][i] );
		        		System.out.print( "\t" );
		        	}
				}
				System.out.print("\n");
			}
			System.out.println();
		}
	}
	
	// Function implementing the on-policy Monte Carlo algorithm
	public List<double[]> onPolicyMC(double epsilon, double discountFactor, int nEpisodes){
		// initialize Q values, return list and an epsilon soft policy
		initQvalues(-1);
		Map<Point, Map<Point, ReturnsSaver>> returnsList = initReturnsList();
		Map<Point, Map<Point, Double>> policy = initEpsilonSoftPolicy();
		// init for storing plot values
		double[] steps = new double[nEpisodes+1];
		double[] xData = new double[nEpisodes+1];
		List<double[]> data = new ArrayList<double[]>();
		for(int i = 0; i < nEpisodes; i++){
			// Generate episode 
			List<List<Point>> episode = generateEpisodeOnPol(policy);
			List<Double> rewards = rewardList;
			
			/*
			 * For each pair s,a appearing in the episode:
			 * - compute return following the first occurrence of s,a
			 * - append return to Returns(s,a)
			 * - Q(s,a) <-- average(Returns(s,a))
			 */
			for( List<Point> stateAct : appearanceList.keySet()){
				Point state = stateAct.get(0);
				Point action = stateAct.get(1);
				int t = episode.indexOf(stateAct);
				double ret = computeReturnFromT(rewards, t, discountFactor);
				returnsList.get(state).get(action).addReturn(ret);
				double averageRet = returnsList.get(state).get(action).getAverageReturn();
				Qvalues.get(state).put(action, averageRet);
			}
			
			/*
			 * For each s in the episode
			 */
			for(Point s : appearanceListS.keySet()){
				Point greedyAction = getArgmaxActionQval(s);
		
				// For every action
				double nActions = policy.get(s).size();
				for(Point a : policy.get(s).keySet()){
					double newProb;
					if(a.equals(greedyAction)){
						newProb = 1 - epsilon + (epsilon/nActions);
					} else{
						newProb = epsilon/nActions;
					}
					policy.get(s).put(a, newProb);
				}
			}
			steps[i] = episode.size();
			xData[i] = i;
		}
		printBoardQ(Qvalues, new Point(5,5));
		printboardQActions(Qvalues, new Point(5,5));
		printboardQmax(Qvalues, new Point(5,5));
		data.add(steps);
		data.add(xData);
		return data;
	}
	
	// Get the maximizing action in state s
	private Point getArgmaxActionQval(Point s){
		Map<Point, Double> actVals = Qvalues.get(s);
		double maxVal = 0;
		int bestActionInd = -1;
		for(int i = 0; i < pred.actions.length; i++){
			double val = actVals.get(move(pred.actions[i]));
			if( val >= maxVal ){
				maxVal = val;
				bestActionInd = i;
			}
		}
		Point bestAction = move(pred.actions[bestActionInd]);
		return bestAction;
	}
	
	// Function to generate an episode for on-policy Monte Carlo
	// using an epsilon soft policy as input
	private List<List<Point>> generateEpisodeOnPol(Map<Point, Map<Point, Double>> policy){
		appearanceList.clear();
		appearanceListS.clear();
		rewardList.clear();
		Point terminalState = new Point(0,0);
		List<List<Point>> episode = new ArrayList<List<Point>>();
		Point s = initS();
		boolean inTerminalState = false;
		while(!inTerminalState){
			// get predator move and store it
			Point action = getActionSoftPol(policy, s);
			episode.add(new ArrayList<Point>(Arrays.asList(s, action)));
			appearanceList.put(new ArrayList<Point>(Arrays.asList(s, action)), true);
			appearanceListS.put(s, action);
			// apply predator move
			Point sTemp = (Point) s.clone();
			sTemp.translate(-1*action.x, -1*action.y);
			sTemp = pred.checkDirections(sTemp);
			Point sPrime = new Point(0,0);
			
			// Observe reward
			int reward = 0;
			// If predator move resulted in terminal state,
			// observe reward of 10 and end episode
			if(sTemp.equals(terminalState)){
				reward = 10;
				inTerminalState = true;
				sPrime = sTemp;
			} else{
				// let prey do move
				sPrime = interactWithEnv(sTemp);
			}
			// store reward
			rewardList.add((double) reward);
			s = sPrime;
		}
		return episode;
	}
	
	// Function for generating an episode for off-poliy Monte Carlo
	// using an epsilon soft policy as input
	// differs from on policy episode generation through the return
	// and how it should be stored
	public ArrayList<ArrayList<Point>> generateEpisodeOffPol( Map<Point, Map<Point, Double>> eSoftPol ) {
		ArrayList<ArrayList<Point>> episode = new ArrayList<ArrayList<Point>>(); 
		//init predator and prey randomly
		Predator pred = new Predator(true);
		Prey prey = new Prey(true);
		while(pred.pos.equals(prey.pos)){
			pred = new Predator(true);
			prey = new Prey(true);
		}
		// get reduced state from prey and pred positions
		Point state = new Point();
		state.x = prey.pos.x - pred.pos.x;
		state.y = prey.pos.y - pred.pos.y;
		state = pred.checkDirections( state );
		boolean notTerminal = true;
		while( notTerminal ) {
			// each timestep add the state
			ArrayList<Point> timeStep = new ArrayList<Point>();
			timeStep.add( state );

			// add the predator move drawn from the epsilon
			// soft policy and apply the move
			Point moveCoordsPred = getActionSoftPol( eSoftPol, state );
			pred.newPos( moveCoordsPred );
			timeStep.add( moveCoordsPred );
			// get new state after move
			Point newState = new Point();
			newState.x = prey.pos.x - pred.pos.x;
			newState.y = prey.pos.y - pred.pos.y;
			newState = pred.checkDirections( newState );
			//if new state equals terminal state store reward of 10
			//on the x coordinate of a point (necessary because of storing)
			Point returnT = new Point(0,0);
			if( newState.equals(new Point(0,0) ) ) {
				returnT.x = 10;
				notTerminal = false;
			} else {
				// if not terminal let prey do move
				String predNear = prey.checkPred( pred );
				String preyMove = prey.getMove( predNear );
				Point moveCoordsPrey = move( preyMove );
				prey.newPos( moveCoordsPrey );
			}
			// add the found reward to the timestep
			timeStep.add( returnT );
			
			// add the timestep to the episode
			episode.add( timeStep ); 
			state = (Point) newState.clone();
		}
		return episode;
	}
	
	// function computing the discounted return 
	// from timestep t to end of on policy episode
	private double computeReturnFromT(List<Double> rewards, int t, double discountFactor){
		double returnFromT = 0;
		int count = 0;
		for(int i = t; i < rewards.size(); i++){
			returnFromT = returnFromT + (Math.pow(discountFactor, count)*rewards.get(i));
			count++;
		}
		
		return returnFromT;
	}
	
	// function computing the discounted return
	// from timestep firstT to the end of off policy episode
	private int getReturn( ArrayList<ArrayList<Point>> episode, int firstT, 
			double discountFactor ) {
		int returnT = 0;
		int count = 0;
		for( int i = firstT; i < episode.size(); i++ ) {
			ArrayList<Point> timeStep = episode.get(i);
			returnT += Math.pow(discountFactor, count) * timeStep.get(2).x;
			count++;
		}
		return returnT;
	}
	
	// Function to initialize the N and D values from the
	// off policy MC update rule for all state-action pairs on zero
	private void initNDvalues( ) {
		int[] directionValues = { -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5 };
		Map<Point, Double> actionVal = new HashMap<Point, Double>();
		for(int i = 0; i < pred.actions.length; i++){
			actionVal.put(move(pred.actions[i]), 0.0);
		}
		for( int i = 0; i < 11; i++ ) {
			for( int j = 0; j < 11; j++ ) {
				Point directionVector = 
						new Point( directionValues[i], directionValues[j] );
				Map<Point, Double> actionVals;
				actionVals = new HashMap<Point, Double>( actionVal );

				Nvalues.put( directionVector, actionVals );
				Dvalues.put( directionVector, actionVals );
			}
		}
	}
	
	// Function to arbitrarily initialize a deterministic policy
	// randomly assigning actions to each state
	private void initDetPolicy( ) {
		Random rand = new Random();
		for( Map.Entry<Point, Map<Point, Double>> entry : Qvalues.entrySet() ) {
			int move = rand.nextInt(5);
			Point state = entry.getKey();
			detPolicy.put( state, move( pred.actions[move] ) );
		}
	}
	
	// Function to compute the weight W from the off policy MC 
	// update rule for each unique state-action pair
	private double computeW( ArrayList<ArrayList<Point>> episode, int firstOcc,
			Map<Point, Map<Point, Double>> eSoftPol ) {
		double w = 1;
		for( int i = firstOcc + 1; i < episode.size(); i++ ) {
			Point state = episode.get(i).get(0);
			Point action = episode.get(i).get(1);
			Map<Point, Double> actions = eSoftPol.get( state );
			double actionProb = actions.get( action );
			w *= 1 / actionProb;
		}
		return w;
	}
	
	// function to select an action from the epsilon soft policy
	// given a certain state
	private Point getActionSoftPol(Map<Point, Map<Point, Double>> policy, Point s){
		Random rand = new Random();
		double chance = rand.nextDouble();
		Map<Point, Double> actionProbs = policy.get(s);
		List<Double> probs = new ArrayList<Double>();
		// get the probabilities for each action from the policy
		// and sum them cumulatively
		probs.add(actionProbs.get(move(pred.actions[0])));
		for( int i = 1; i < pred.actions.length; i++ ) {
			probs.add(probs.get(i-1) + actionProbs.get(move(pred.actions[i])));
		}
		// use the random double [0,1) to choose an action
		int actionInd = -1;
		for(int i = 0; i < probs.size(); i++){
			if( chance < probs.get(i) ){
				actionInd = i;
				break;
			}
		}
		Point action = move(pred.actions[actionInd]);

		return action;
	}
	
	// Function to initialize the returns list for the
	// on policy monte carlo algorithm
	private Map<Point, Map<Point, ReturnsSaver>> initReturnsList(){
		Map<Point, Map<Point, ReturnsSaver>> returnsList = 
				new HashMap<Point, Map<Point, ReturnsSaver>>();
		for(Point state : Qvalues.keySet()){
			Map<Point, ReturnsSaver> retMap = new HashMap<Point, ReturnsSaver>();
			for( Point action: Qvalues.get(state).keySet()){
				retMap.put(action, new ReturnsSaver());
			}
			returnsList.put(state, retMap);
		}
		
		return returnsList;
	}
	
	// Function to initialize an epsilon soft policy
	// assigning random probabilities (> 1) to each action
	// summing probabilities for all actions to 1
	private Map<Point, Map<Point, Double>> initEpsilonSoftPolicy(){
		Map<Point, Map<Point, Double>> eSoftPol = new HashMap<Point, Map<Point, Double>>();
		for(Entry<Point, Map<Point, Double>> stateActVal : Qvalues.entrySet()){
			double[] probs = getNormalizedProbDist(stateActVal.getValue().size());
			int counter = 0;
			Map<Point, Double> actVal = new HashMap<Point, Double>();
			for(Point act : stateActVal.getValue().keySet() ){
				actVal.put(act, probs[counter]);
				counter++;
			}
			eSoftPol.put(stateActVal.getKey(), actVal);
		}
		
		return eSoftPol;
	}
	
	// Function to get normalized probabilities (summing to 1)
	private double[] getNormalizedProbDist(int n){
		Random rand = new Random();
		double[] probDist = new double[n];
		double sum = 0;
		for(int i = 0; i < n; i++){
			probDist[i] = rand.nextDouble();
			sum = sum + probDist[i];
		}
		for(int i = 0; i < n; i++){
			probDist[i] = probDist[i]/sum;
		}
		
		return probDist;
	}
		
	// Function implementing the Q-learning algorithm
	public List<double[]> qlearning(double alpha, double discountFactor, int nEpisodes, boolean greedy,
			double initQval, double epsilon, double temperature){
		initQvalues(initQval);
		Point terminalState = new Point(0,0);
		double[] steps = new double[nEpisodes+1];
		double[] xData = new double[nEpisodes+1];
		for( int i = 0; i < nEpisodes; i++){
			Point s = initS();
			boolean inTerminalState = false;
			int stepCounter = 0;
			while(!inTerminalState){
				stepCounter = stepCounter + 1;
				Point action;
				if(greedy){
					action = getActionGreedy(epsilon, s);
				}
				else{
					action = getActionSoftmax(temperature, s);
				}
				Point sTemp = (Point) s.clone();
				sTemp.translate(-1*action.x, -1*action.y);
				sTemp = pred.checkDirections(sTemp);
				Point sPrime = new Point(0,0);
				
				// Observe reward
				int reward = 0;
				// If predator move resulted in terminal state,
				// observe reward of 10, update Qvalue and end the episode
				if(sTemp.equals(terminalState)){
					reward = 10;
					inTerminalState = true;
					sPrime = sTemp;
				}
				else{
					sPrime = interactWithEnv(sTemp);
				}
				
				// Compute Q value for current state s
				double qval = computeQvalueQL(s, action, alpha, 
						discountFactor, sPrime, reward);
				Qvalues.get(s).put(action, qval);
				
				// s is sPrime
				s = sPrime;
			}
			steps[i] = stepCounter;
			xData[i] = i;
		}
		printBoardQ(Qvalues, new Point(5,5));
		printboardQActions(Qvalues, new Point(5,5));
		printboardQmax(Qvalues, new Point(5,5));
		List<double[]> plotData = new ArrayList<double[]>();
		plotData.add(steps);
		plotData.add(xData);
		
		return plotData;
	}
	
	// Compute Q-value in Qlearning for state s
	private double computeQvalueQL(Point s, Point action, double alpha, 
			double discountFactor, Point sPrime, int reward){
		double oldqval = Qvalues.get(s).get(action);
		Set<Point> sPrimeActions = Qvalues.get(sPrime).keySet();
		double maxPrimeQval = 0;
		for(Point actionPrime : sPrimeActions){
			if(Qvalues.get(sPrime).get(actionPrime) >= maxPrimeQval){
				maxPrimeQval = Qvalues.get(sPrime).get(actionPrime);
			}
		}
		double qval = oldqval + alpha*(reward + 
				(discountFactor*maxPrimeQval) - oldqval);
		
		return qval;
	}
	
	// Retrieve next state when interacting with environment
	// (Prey does a move)
	private Point interactWithEnv(Point sTemp){
		Point sPrime;
		Point predNear = prey.predNear(sTemp);
		Point actionPrey = prey.getMove(predNear);
		sTemp.translate(actionPrey.x, actionPrey.y);
		sPrime = pred.checkDirections(sTemp);
		
		return sPrime;
	}
	
	// Get epsilon-greedy action for predator
	private Point getActionGreedy(double epsilon, Point state){
		Random rand = new Random();
		double chance = rand.nextDouble();
		Point action = new Point();

		// Select random action with probability epsilon
		if(chance < epsilon){
			//System.out.println("Random action");
			action = move(pred.actions[rand.nextInt(pred.actions.length)]);
		}
		// Select optimal action
		else{
			double maxVal = 0;
			Map<Point, Double> valActFromState = Qvalues.get(state);
			for(Map.Entry<Point, Double> entry : valActFromState.entrySet()){
				if(entry.getValue() >= maxVal){
					action = entry.getKey();
					maxVal = entry.getValue();
				}
			}
		}
		
		return action;
	}
		
	// Function to select an action according to softmax algorithm
	// dependent on the temperature term the Q values will have 
	// more (lower temperature) or less (higher temperature) influence
	// on the probabilities of the corresponding actions
	private Point getActionSoftmax(double temperature,  Point state ) {
		Random rand = new Random();
		double chance = rand.nextDouble();
		ArrayList<Double> softmaxProbs = softmaxProbabilities( temperature, state );
		int softmaxMove = -1;
		for( int i = 0; i < softmaxProbs.size(); i++ ) {
			if( chance < softmaxProbs.get(i) ) {
				softmaxMove = i;
				break;
			}
		}
		Point move = move( pred.actions[softmaxMove] );
		return move;
	}
	
	// Function to compute the softmax probabilities for a certain state
	private ArrayList<Double> softmaxProbabilities( double temperature, Point state ) {
		ArrayList<Double> softmaxProbs = new ArrayList<Double>();
		Map<Point, Double> valActFromState = Qvalues.get( state );
		double sum = 0;
		for( Point act : valActFromState.keySet() ) {
			sum += Math.exp( valActFromState.get( act ) / temperature );
		}
		
		// compute each individual softmax probability for actions
		for( int i = 0; i < valActFromState.entrySet().size(); i++ ) {
			double actVal = valActFromState.get(move(pred.actions[i]));
			double prob = Math.exp( actVal / temperature ) / sum;
			softmaxProbs.add( prob );
		}
		// sum cumulatively to get chance boundaries
		for( int i = 1; i < softmaxProbs.size(); i++ ) {
			softmaxProbs.set( i, softmaxProbs.get(i-1) + softmaxProbs.get(i) );
		}
		return softmaxProbs;		
	}
	
	// get an arbitrary starting state, that is not the terminal state
	private Point initS(){
		Point initialState = new Point();
		boolean nonTerminalInit = false;
		while( !nonTerminalInit ) {
			Random rand = new Random();
			initialState = statesArray[rand.nextInt(statesArray.length)];
			if( !initialState.equals(new Point(0,0) )) {
				nonTerminalInit = true;
			}
		}
		return initialState;
	}
	
	// Initialize reduced state space
	private void initQvalues(double initialQvalues) {
		int[] directionValues = { -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5 };
		Map<Point, Double> actionValterm = new HashMap<Point, Double>(); // for terminal state
		// Random initial Q values
		if(initialQvalues < 0.0){
			Random rand = new Random();
			for( int i = 0; i < 11; i++ ) {
				for( int j = 0; j < 11; j++ ) {
					Point directionVector = 
							new Point( directionValues[i], directionValues[j] );
					Map<Point, Double> actionQVal;
					if( directionVector.equals( new Point(0,0) ) ) {
						for(int k = 0; k < pred.actions.length; k++){
							actionValterm.put(move(pred.actions[k]), 0.0);
						}
						actionQVal = new HashMap<Point, Double>(actionValterm);
					} else {
						Map<Point, Double> actionVal = new HashMap<Point, Double>();
						for(int k = 0; k < pred.actions.length; k++){
							actionVal.put(move(pred.actions[k]), (double) rand.nextInt(21));
						}
						actionQVal = new HashMap<Point, Double>(actionVal);
					}
					Qvalues.put( directionVector, actionQVal );
				}
			}
		} else{
			Map<Point, Double> actionVal = new HashMap<Point, Double>();
			for(int i = 0; i < pred.actions.length; i++){
				actionValterm.put(move(pred.actions[i]), 0.0);
				actionVal.put(move(pred.actions[i]), initialQvalues);
			}
			for( int i = 0; i < 11; i++ ) {
				for( int j = 0; j < 11; j++ ) {
					Point directionVector = 
							new Point( directionValues[i], directionValues[j] );
					Map<Point, Double> actionQVal;
					if( directionVector.equals( new Point(0,0) ) ) {
						actionQVal = new HashMap<Point, Double>(actionValterm);
					} else {
						actionQVal = new HashMap<Point, Double>(actionVal);
					}
					Qvalues.put( directionVector, actionQVal );
				}
			}
		}
		Set<Point> states = Qvalues.keySet();
		statesArray = states.toArray( new Point[states.size()] );
	}
	
	// Function implementing the Sarsa algorithm
	public void Sarsa( double discountFactor, double learningRate, int nEpisodes,
			boolean useGreedy, double epsilon, double temperature ) {
		// optimistically init the Q values
		initQvalues( 15.0 );
		for( int i = 0; i < nEpisodes; i++ ) {
			Point s = initS();
			Point action;
			// select an action either e greedy or softmax
			if( useGreedy )
				action = getActionGreedy( epsilon, s );
			else
				action = getActionSoftmax( temperature, s );
			boolean terminalState = false;
			int nrOfSteps = 0;
			// run until end of episode
			while( !terminalState ) {
				// do predator action with chosen move
				Point sPrime = (Point) s.clone();
				sPrime.translate( -1*action.x, -1*action.y );
				sPrime = pred.checkDirections( sPrime );
				nrOfSteps++;
				// check if reward is gained and thus terminal
				int reward = 0;
				if( sPrime.equals( new Point(0,0) ) ) {
					reward = 10;
					terminalState = true;
				} else {
					// if not, let prey do move
					sPrime = interactWithEnv( sPrime );
					sPrime = pred.checkDirections( sPrime );
				}
				// get a possible action in the new state
				Point actionPrime;
				if( useGreedy )
					actionPrime = getActionGreedy( epsilon, sPrime );
				else
					actionPrime = getActionSoftmax( temperature, sPrime );
				// update the q value of the current state using the 
				// possible move in the new state
				Map<Point, Double> currentStateQvals = Qvalues.get( s );
				double currentQval = currentStateQvals.get( action );
				Map<Point, Double> newStateQvals = Qvalues.get( sPrime );
				double newQval = newStateQvals.get( actionPrime );
				double updatedValue = currentQval + 
						learningRate * ( reward + discountFactor * newQval - currentQval );
				// store the updated q value
				currentStateQvals.put( action, updatedValue );
				Qvalues.put(s, currentStateQvals);
				// set the next action and state as the current state and action
				s = (Point) sPrime.clone();
				action = (Point) actionPrime.clone();
				//printBoardQ( Qvalues, s );
			}
			System.out.println( nrOfSteps );
			
		}
		// print values
		printBoardQ( Qvalues, new Point(5,5) );
		printboardQmax( Qvalues, new Point(5,5));
		printboardQActions( Qvalues, new Point(5,5) );
	}
	
	// check if terminal state
	public boolean checkTerminalState( Point state ) {
		if ( calcDistance( state ) == 0 ) {
			return true;
		}
		return false;
	}

	// Function to update the Q values for the off policy MC algorithm
	private void updateQMC( ArrayList<ArrayList<Point>> episode, int tau, 
			Map<Point, Map<Point, Double>> eSoftPol, double discountFactor ) {
		Map<List<Point>, Integer> firstOccurs = findFirstOccurs( episode, tau );
		
		// store the number of times a certain state is updated
		for( Map.Entry<List<Point>, Integer> entry : firstOccurs.entrySet() ) {
			Point key = entry.getKey().get(0);
			if( !stateCounts.containsKey( key ) ) {
				stateCounts.put( key, 1 );
			} else {
				stateCounts.put( key, stateCounts.get(key) +  1 );
			}
		}
		
		for( int i = tau; i < episode.size(); i++ ) {
			Point state = episode.get(i).get(0);
			Point action = episode.get(i).get(1);
			List<Point> key = new ArrayList<Point>();
			key.add(state); key.add(action);
			int firstOcc = firstOccurs.get( key );
			if( firstOcc == i ) {
				int returnT = getReturn( episode, firstOcc, discountFactor );
				double w = computeW( episode, firstOcc, eSoftPol );
				Map<Point, Double> Nval = 
						new HashMap<Point, Double>( Nvalues.get( state ) );
				Nval.put( action, Nval.get(action) + (double) w*returnT );
				Nvalues.put( state, Nval );
				Map<Point, Double> Dval =
						new HashMap<Point, Double>( Dvalues.get( state ) );
				Dval.put(action, Dval.get(action) + w );
				Dvalues.put( state, Dval );
				
				Qvalues.get(state).put( action, Nvalues.get( state ).get( action ) / 
						Dvalues.get( state ).get( action )  );
			}
		}
	}
	
	// Function to print the number of times a state is updated
	public void printStateCount( Map<Point, Integer> map, Point preyLoc ) {
		int[][] board = new int[11][11];
		for( Map.Entry<Point, Integer> entry : map.entrySet() ) {
			Point state = entry.getKey();
			int action = map.get( state );
			Point newState = (Point) preyLoc.clone();
			newState.x -= state.x;
			newState.y -= state.y;
			newState = checkLoc( newState );
			board[newState.y][newState.x] = action;
		}
		for( int[] row : board ) {
	        for( int r : row ) {
	        	System.out.printf( "%d\t", r );
	        }
	        System.out.println();
	    }
		System.out.println();
	}
	
	// Function to find the first occurrences of each unique 
	// state in the tail of an episode
	private Map<List<Point>, Integer> findFirstOccurs( ArrayList<ArrayList<Point>> episode, int tau ) {
		Map<List<Point>, Integer> firstOccurs = new HashMap<List<Point>, Integer>();
		for( int i = tau; i < episode.size(); i++ ) {
			Point state = episode.get(i).get(0);
			Point action = episode.get(i).get(1);
			List<Point> key = new ArrayList<Point>();
			key.add(state); key.add(action);
			if( !firstOccurs.containsKey(key) ) {
				firstOccurs.put(key, i);
			}
		}
		return firstOccurs;
	}
	
	// Function to update the current deterministic policy
	// with the actions with the maximizing Q values for each state
	private void updateDetPolicy() {
		for( Map.Entry<Point, Map<Point, Double>> entry : Qvalues.entrySet() ) {
			Point state = entry.getKey();
			if( state.equals(new Point(0,0)))
				continue;
			Map<Point, Double> actions = entry.getValue();
			double bestVal = 0;
			int actionIndex = -1;
			for( int i = 0; i < actions.size(); i++ ) {
				double value = actions.get( move( pred.actions[i] ) );
				if( value >= bestVal ) {
					bestVal = value;
					actionIndex = i;
				}
			}
			detPolicy.put( state, move(pred.actions[actionIndex] ));
		}
	}
	
	
	// function to print the deterministic policy from off policy MC 
	public void printDetPolicy( Map<Point, Point> map, Point preyLoc ) {
		String[][] board = new String[11][11];
		for( Map.Entry<Point, Point> entry : map.entrySet() ) {
			Point state = entry.getKey();
			Point move = entry.getValue();
			Point nextLoc = (Point) preyLoc.clone();
			nextLoc.x -= state.x;
			nextLoc.y -= state.y;
			nextLoc = checkLoc( nextLoc );
			String bestAction = "";
			if(!nextLoc.equals(preyLoc)){
				if( move.equals(new Point(-1,0) ) ) {
					bestAction = "WEST";
				} else {
					if( move.equals(new Point(1,0) ) ) {
						bestAction = "EAST";
					} else {
						if( move.equals(new Point(0,-1) ) ) {
							bestAction = "NORTH";
						} else {
							if( move.equals(new Point(0,1) ) ) {
								bestAction = "SOUTH";
							} else {
								if( move.equals(new Point(0,0) ) ) {
									bestAction = "WAIT";
								}
							}
						}
					}
				}
			}
			board[nextLoc.y][nextLoc.x] = bestAction;
		}
		for( String[] row : board ) {
	        for( String r : row ) {
	        	System.out.printf( "%s\t", r );
	        }
	        System.out.println();
	    }
		System.out.println();
	}
	
	// Function implementing off-policy MC algorithm
	public List<double[]> offPolicyMonteCarlo( double initQvalues, double discountFactor, int nEpisodes ) {
		List<double[]> plotData = new ArrayList<double[]>();
		double episodeIndex[] = new double[nEpisodes];
		double stepCount[] = new double[nEpisodes];
		initQvalues( initQvalues );
		initNDvalues();
		initDetPolicy();
		for( int k = 0; k < nEpisodes; k++ ) { 
			Map<Point, Map<Point, Double>> eSoftPol = initEpsilonSoftPolicy();
			ArrayList<ArrayList<Point>> episode = generateEpisodeOffPol( eSoftPol );
			for( int i = episode.size() - 1; i >= 0; i-- ) {
				ArrayList<Point> timeStep = episode.get( i );
				Point state = timeStep.get( 0 );
				Point action = timeStep.get( 1 );
				Point detPolAction = detPolicy.get( state );
				
				if( !action.equals( detPolAction ) ) {
					updateQMC( episode, i, eSoftPol, discountFactor );
					break;
				}
			}
			
			updateDetPolicy();
			// values for the plots in the report
			episodeIndex[k] = k;
			int steps = start1( false );
			stepCount[k] = steps;
			
		}
		plotData.add(stepCount);
		plotData.add(episodeIndex);
		printDetPolicy( detPolicy, new Point(5,5) );
		printStateCount( stateCounts, new Point(5,5) );

		return plotData;
	}
}